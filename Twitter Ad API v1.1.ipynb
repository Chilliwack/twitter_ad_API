{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start working with the Twitter Ads API you'll first need to install the Twitter Ads SDK. Once that is installed it's also recommended to install and use Twurl to handle the API authentication process.\n",
    "\n",
    "Twitters Ads Insight SDK for Python - http://twitterdev.github.io/twitter-python-ads-sdk/   \n",
    "Twitters Ads Platform Tools - https://github.com/twitterdev/ads-platform-tools\n",
    "    \n",
    "Setting up Twurl:\n",
    "- https://github.com/twitter/twurl\n",
    "- https://dev.twitter.com/ads/tutorials/using-twurl  \n",
    "\n",
    "Ads Analytics API Best Practices:\n",
    "- https://dev.twitter.com/ads/analytics/best-practices\n",
    "- https://dev.twitter.com/ads/analytics/metrics-and-segmentation\n",
    "- https://dev.twitter.com/ads/analytics/metrics-derived\n",
    "- https://dev.twitter.com/ads/tutorials/hierarchy-and-terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Ad API ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# default notebook setup\n",
    "MAX_ROWS = 10\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "# import the needed modules\n",
    "import time\n",
    "from twitter_ads.client import Client\n",
    "from twitter_ads.campaign import LineItem\n",
    "from twitter_ads.enum import METRIC_GROUP\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defined functions that will be utilized later on\n",
    "\n",
    "# import requests\n",
    "import oauth2 as oauth\n",
    "import yaml\n",
    "# import urllib\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "# import pytz\n",
    "import datetime\n",
    "import argparse\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Handles the API authentication. See https://developer.twitter.com/en/docs/tutorials/using-twurl & https://github.com/twitter/twurl for more info.\n",
    "def twurlauth():\n",
    "    with open(os.path.expanduser('~/.twurlrc'), 'r') as f:\n",
    "        contents = yaml.load(f)\n",
    "        f.close()\n",
    "\n",
    "    default_user = contents[\"configuration\"][\"default_profile\"][0]\n",
    "\n",
    "    CONSUMER_KEY = contents[\"configuration\"][\"default_profile\"][1]\n",
    "    CONSUMER_SECRET = contents[\"profiles\"][default_user][CONSUMER_KEY][\"consumer_secret\"]\n",
    "    USER_OAUTH_TOKEN = contents[\"profiles\"][default_user][CONSUMER_KEY][\"token\"]\n",
    "    USER_OAUTH_TOKEN_SECRET = contents[\"profiles\"][default_user][CONSUMER_KEY][\"secret\"]\n",
    "\n",
    "    return CONSUMER_KEY, CONSUMER_SECRET, USER_OAUTH_TOKEN, USER_OAUTH_TOKEN_SECRET\n",
    "\n",
    "# Handles the request\n",
    "def request(user_twurl, http_method, headers, url):\n",
    "    CONSUMER_KEY = user_twurl[0]\n",
    "    CONSUMER_SECRET = user_twurl[1]\n",
    "    USER_OAUTH_TOKEN = user_twurl[2]\n",
    "    USER_OAUTH_TOKEN_SECRET = user_twurl[3]\n",
    "\n",
    "    consumer = oauth.Consumer(key=CONSUMER_KEY, secret=CONSUMER_SECRET)\n",
    "    token = oauth.Token(key=USER_OAUTH_TOKEN, secret=USER_OAUTH_TOKEN_SECRET)\n",
    "    client = oauth.Client(consumer, token)\n",
    "\n",
    "    header_list = {}\n",
    "    if headers:\n",
    "        for i in headers:\n",
    "            (key, value) = i.split(': ')\n",
    "            if key and value:\n",
    "                header_list[key] = value\n",
    "\n",
    "    response, content = client.request(url, method=http_method, headers=header_list)\n",
    "\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "    except:\n",
    "        data = None\n",
    "    return response, data\n",
    "\n",
    "# Gets the data\n",
    "def get_data(user_twurl, http_method, headers, url):\n",
    "    data = []\n",
    "\n",
    "    res_headers, response = request(user_twurl, http_method, headers, url)\n",
    "\n",
    "    if res_headers['status'] != '200':\n",
    "        print('ERROR: query failed, cannot continue: %s' % url)\n",
    "        sys.exit(0)\n",
    "\n",
    "    if response and 'data' in response:\n",
    "        data += response['data']\n",
    "\n",
    "    while 'next_cursor' in response and response['next_cursor'] is not None:\n",
    "        cursor_url = url + '&cursor=%s' % response['next_cursor']\n",
    "        res_headers, response = request(user_twurl, http_method, headers, cursor_url)\n",
    "\n",
    "        if response and 'data' in response:\n",
    "            data += response['data']\n",
    "\n",
    "    return data\n",
    "\n",
    "# Reformat timestamps\n",
    "def format_timestamp(timestamp):\n",
    "    return datetime.datetime.strptime(timestamp, '%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# Check the datetime filtering of the data\n",
    "def check(data, start_time, end_time, filter_field=None, filter_data=[]):\n",
    "\n",
    "    d = []\n",
    "\n",
    "    if data and len(data) > 0:\n",
    "        for i in data:\n",
    "            if 'end_time' in i and i['end_time'] and format_timestamp(i['end_time']) < start_time:\n",
    "                continue\n",
    "            elif ('start_time' in i and i['start_time'] and\n",
    "                  format_timestamp(i['start_time']) > end_time):\n",
    "                continue\n",
    "            elif i['deleted'] and format_timestamp(i['updated_at']) < start_time:\n",
    "                continue\n",
    "#             elif i['paused'] and format_timestamp(i['updated_at']) < start_time:\n",
    "#                 continue\n",
    "            elif filter_field and i[filter_field] not in filter_data:\n",
    "                continue\n",
    "            else:\n",
    "                d.append(i['id'])\n",
    "\n",
    "    return d\n",
    "\n",
    "def fetcher(job_id):\n",
    "    import sys\n",
    "    seconds = 10\n",
    "    tries = 3\n",
    "    print 'Job is running..'\n",
    "    while tries != 0:\n",
    "        for i in xrange(seconds,0,-1):\n",
    "            time.sleep(1)\n",
    "        # fetch the job result of the specified async job ID\n",
    "        async_stats_job_result = LineItem.async_stats_job_result(account, job_id)\n",
    "        if async_stats_job_result['status'] == 'SUCCESS':\n",
    "            print 'Fetching the response of the specified async job ID'\n",
    "            # fetch the response of the specified async job ID\n",
    "            async_data = LineItem.async_stats_job_data(account, async_stats_job_result['url'])\n",
    "            print 'Done'\n",
    "            return async_data\n",
    "        else:\n",
    "            tries -= 1\n",
    "            print 'Job is still processing. %s tries left' % tries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the request ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Create a new Ads account on the SANDBOX environment and return the account_id\n",
    "\n",
    "# headers = None\n",
    "# url = 'https://ads-api-sandbox.twitter.com/2/accounts/' # SANDBOX_DOMAIN\n",
    "# response, content = request(user_twurl, 'POST', headers, url)\n",
    "# sandbox_account_id = content['data']['id']\n",
    "# print 'Sandbox Account ID:', sandbox_account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API Authentication\n",
    "user_twurl = twurlauth() # API authentication using the twurl function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_time: 2017-07-01 00:00:00\n",
      "end_time: 2017-07-31 22:59:59\n",
      "Domain: https://ads-api-sandbox.twitter.com\n",
      "Account_id: gq183a\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters for the request\n",
    "\n",
    "# DOMAIN = 'https://ads-api.twitter.com'     # PROD_DOMAIN\n",
    "DOMAIN = 'https://ads-api-sandbox.twitter.com'     # SANDBOX_DOMAIN\n",
    "\n",
    "ACCOUNT_ID = 'gq183a'    #PROD ACCOUNT_ID\n",
    "# ACCOUNT_ID = 'gq183a'     #SANDBOX_DOMAIN\n",
    "headers = None\n",
    "\n",
    "# Set the timeframe to pull\n",
    "start_time = datetime.datetime.strptime('2017-07-01', '%Y-%m-%d')\n",
    "end_time = datetime.datetime.strptime('2017-07-31', '%Y-%m-%d')\n",
    "granularity = 'HOUR'\n",
    "\n",
    "# Adds a day and subtracts a second so you get the full 23:59:59 h:m:s end_time day\n",
    "end_time += datetime.timedelta(hours=23)\n",
    "end_time -= datetime.timedelta(seconds=1)\n",
    "\n",
    "print 'start_time:', start_time\n",
    "print 'end_time:', end_time\n",
    "print 'Domain:', DOMAIN\n",
    "print 'Account_id:', ACCOUNT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have access to account ID:gq183a\n"
     ]
    }
   ],
   "source": [
    "# Check that we have access to this :ACCOUNT_ID\n",
    "\n",
    "resource_path = '/2/accounts/%s' % ACCOUNT_ID\n",
    "data = get_data(user_twurl, 'GET', headers, DOMAIN + resource_path)\n",
    "\n",
    "if len(data) == 0:\n",
    "    print('ERROR: Could not locate :account ID %s' % ACCOUNT_ID)\n",
    "    sys.exit(0)\n",
    "else:\n",
    "    print('You have access to account ID:%s' % ACCOUNT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conduct stats check for the account based on the parameters set ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats check for :account_id gq183a\n",
      "--------------------------------------------------------------------------------\n",
      "Start time:\t2017-07-01 00:00:00\n",
      "End time:\t2017-07-31 22:59:59\n",
      "--------------------------------------------------------------------------------\n",
      "Pre-filtered data:\t\t1\n",
      "Funding instruments:\t\t0\n",
      "Pre-filtered data:\t\t0\n",
      "Campaigns:\t\t\t0\n",
      "Pre-filtered data:\t\t0\n",
      "Line items:\t\t\t0\n",
      "Pre-filtered data:\t\t0\n",
      "Promoted Tweets:\t\t0\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Stats check for :account_id %s\" % ACCOUNT_ID)\n",
    "print '-'*80\n",
    "print('Start time:\\t%s' % start_time)\n",
    "print('End time:\\t%s' % end_time)\n",
    "print '-'*80\n",
    "\n",
    "# fetch funding instruments\n",
    "resource_path = '/2/accounts/%s/funding_instruments?with_deleted=true&count=1000' % ACCOUNT_ID\n",
    "data = get_data(user_twurl, 'GET', headers, DOMAIN + resource_path)\n",
    "\n",
    "# filter funding instruments\n",
    "print(\"Pre-filtered data:\\t\\t%s\" % len(data))\n",
    "funding_instruments = check(data, start_time, end_time)\n",
    "print(\"Funding instruments:\\t\\t%s\" % len(funding_instruments))\n",
    "\n",
    "# fetch campaigns\n",
    "resource_path = '/2/accounts/%s/campaigns?with_deleted=true&count=1000' % ACCOUNT_ID\n",
    "data = get_data(user_twurl, 'GET', headers, DOMAIN + resource_path)\n",
    "\n",
    "# filter campaigns\n",
    "print(\"Pre-filtered data:\\t\\t%s\" % len(data))\n",
    "campaigns = check(data, start_time, end_time, 'funding_instrument_id', funding_instruments)\n",
    "print(\"Campaigns:\\t\\t\\t%s\" % len(campaigns))\n",
    "\n",
    "# fetch line items\n",
    "resource_path = '/2/accounts/%s/line_items?with_deleted=true&count=1000' % ACCOUNT_ID\n",
    "data = get_data(user_twurl, 'GET', headers, DOMAIN + resource_path)\n",
    "\n",
    "# filter line items\n",
    "print(\"Pre-filtered data:\\t\\t%s\" % len(data))\n",
    "line_items = check(data, start_time, end_time, 'campaign_id', campaigns)\n",
    "print(\"Line items:\\t\\t\\t%s\" % len(line_items))\n",
    "\n",
    "# fetch promoted_tweets\n",
    "resource_path = '/2/accounts/%s/promoted_tweets?with_deleted=true&count=1000' % ACCOUNT_ID\n",
    "data = get_data(user_twurl, 'GET', headers, DOMAIN + resource_path)\n",
    "\n",
    "# filter promoted_tweets\n",
    "print(\"Pre-filtered data:\\t\\t%s\" % len(data))\n",
    "promoted_tweets = check(data, start_time, end_time, 'line_item_id', line_items)\n",
    "print(\"Promoted Tweets:\\t\\t%s\" % len(promoted_tweets))\n",
    "print '-'*80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and make the async report request ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job is running..\n",
      "Fetching the response of the specified async job ID\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# initialize the client\n",
    "client = Client(user_twurl[0], user_twurl[1], user_twurl[2], user_twurl[3])\n",
    "\n",
    "# load the advertiser account instance available to the current access token\n",
    "account = client.accounts(ACCOUNT_ID)\n",
    "\n",
    "# limit request count and grab the first 20 line items from Cursor\n",
    "line_items = list(account.line_items(None, count=1000))[:20]\n",
    "\n",
    "# the list of metrics we want to fetch, for a full list of possible metrics\n",
    "# see: https://dev.twitter.com/ads/analytics/metrics-and-segmentation\n",
    "metric_groups = [METRIC_GROUP.BILLING,\n",
    "                 METRIC_GROUP.ENGAGEMENT,\n",
    "                 METRIC_GROUP.MEDIA,\n",
    "                 METRIC_GROUP.VIDEO\n",
    "#                  METRIC_GROUP.WEB_CONVERSION\n",
    "                ]\n",
    "\n",
    "# fetching async stats on the instance\n",
    "ids = map(lambda x: x.id, line_items)\n",
    "\n",
    "queued_job = LineItem.queue_async_stats_job(account,\n",
    "                                            ids,\n",
    "                                            metric_groups,\n",
    "                                            entity='LineItem',\n",
    "                                            start_time=start_time,\n",
    "                                            end_time=end_time,\n",
    "                                            granularity=granularity)\n",
    "\n",
    "# get the job_id:\n",
    "job_id = queued_job['id']\n",
    "\n",
    "# fetch the response of the specified async job ID using the fetcher function and assign the API response to new 'response' var\n",
    "response = fetcher(job_id)\n",
    "\n",
    "# # fetch the results of the specific async job ID directly\n",
    "# async_stats_job_result = LineItem.async_stats_job_result(account, job_id)\n",
    "# async_stats_job_result['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # GET stats/jobs/summaries\n",
    "# # Retrieve a summary of each existing asynchronous analytics job associated with the client app ID making the request. \n",
    "# # This endpoint is meant for internal / debugging purposes only.\n",
    "\n",
    "# resource_path = '/2/stats/jobs/summaries'\n",
    "# res_headers, response = request(user_twurl, 'GET', headers, 'https://ads-api.twitter.com' + resource_path)\n",
    "# response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the response ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Inspect the 'data' value in the response\n",
    "# print 'start_time:', response['request']['params']['start_time']\n",
    "# print 'end_time' , response['request']['params']['end_time']\n",
    "# from pprint import pprint\n",
    "# pprint(response['data'][0] , depth=6)     # inspect first 'row'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First create a dictionary of dimensional data at the line_item.id level to match up below with the metric data\n",
    "from collections import defaultdict\n",
    "\n",
    "# create a dict to contain line_item dims\n",
    "line_dims = defaultdict(list)\n",
    "for line_item in line_items:\n",
    "    line_dims[line_item.id].append((line_item.name, line_item.campaign_id))\n",
    "    \n",
    "# create a dict to contain campaign level dims\n",
    "camp_dims = defaultdict(list)\n",
    "for campaign in account.campaigns():\n",
    "    camp_dims[campaign.id].append((campaign.currency, campaign.name))\n",
    "\n",
    "# # create a dict to contain media dims\n",
    "# media_dims = {}\n",
    "# for item in account.media_creatives():\n",
    "#     media_dims[item.line_item_id] = (item.media_creative_ids,)\n",
    "    \n",
    "# bring data into the line_dims dict from camp_dims dict\n",
    "for line_item_id, values in line_dims.items():\n",
    "    campaign_id = values[0][1]\n",
    "    if campaign_id in camp_dims:\n",
    "        campaign_currency = camp_dims[campaign_id][0][0]\n",
    "        campaign_name = camp_dims[campaign_id][0][1]\n",
    "        line_dims[line_item_id].append((campaign_currency, campaign_name))\n",
    "    if line_item_id in media_dims:\n",
    "        media_creative_id = media_dims[line_item_id]\n",
    "    else:\n",
    "        media_creative_id = None\n",
    "    line_dims[line_item_id].append(media_creative_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a time series to use as the index to the dataframe based on the times returned in the response\n",
    "start = datetime.datetime.strptime(response['request']['params']['start_time'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "end = datetime.datetime.strptime(response['request']['params']['end_time'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "end -= datetime.timedelta(hours=1)\n",
    "\n",
    "dates = pd.date_range(start=start, end=end, freq='H')\n",
    "\n",
    "# Build the dataframe(s)\n",
    "entries = []\n",
    "for entry in response['data']:\n",
    "    data = entry['id_data'][0]['metrics']    \n",
    "    # Pull in the keys for the column headers\n",
    "    columns = data.keys()\n",
    "    # Add additional columns\n",
    "    add_columns = ['date',\n",
    "                   'advertiser',\n",
    "                   'advertiser_id',\n",
    "                   'advertiser_currency',\n",
    "                   'insertion_order',\n",
    "                   'insertion_order_id',\n",
    "                   'line_item',\n",
    "                   'line_item_id',\n",
    "                   'creative_id',\n",
    "                  ]\n",
    "    columns.extend(add_columns)\n",
    "    # Create a dataframe where each row equals granularity\n",
    "    df = pd.DataFrame(data, index=dates, columns=columns)\n",
    "    # Resample the granularity of the dataframe timeseries to days\n",
    "    df = df.resample('D').sum()\n",
    "    \n",
    "    # Create additional columns\n",
    "    df['click_rate'] = (df['clicks'] / df['impressions'])\n",
    "    df['advertiser'] = account.name\n",
    "    df['advertiser_id'] = account.id\n",
    "    df['advertiser_currency'] = line_dims[entry['id']][1][0]\n",
    "    df['insertion_order'] = line_dims[entry['id']][1][1]\n",
    "    df['insertion_order_id'] = line_dims[entry['id']][0][1]\n",
    "    df['line_item'] = line_dims[entry['id']][0][0]\n",
    "    df['line_item_id'] = entry['id']\n",
    "    df['creative_id'] = line_dims[entry['id']][2]\n",
    "    df['billed_charge_local_micro'] = (df['billed_charge_local_micro'] / 1000000) # spend\n",
    "\n",
    "    # Append dataframe to list\n",
    "    entries.append(df)\n",
    "    \n",
    "# Concat into larger dataframe \n",
    "dataframe = pd.concat(entries)\n",
    "\n",
    "# # Preview API response dataframe\n",
    "# dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take the dataframe and build the report needed ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Columns needed for the report\n",
    "report_columns = ['advertiser',\n",
    "                'advertiser_id',\n",
    "                'advertiser_currency',\n",
    "                'insertion_order',\n",
    "                'insertion_order_id',\n",
    "                'line_item',\n",
    "                'line_item_id',\n",
    "                'creative',\n",
    "                'creative_id',\n",
    "                'impressions',\n",
    "                'qualified_impressions',\n",
    "                'clicks',\n",
    "                'click_rate',\n",
    "                'total_conversions',\n",
    "                'post_click_conversions',\n",
    "                'revenue',\n",
    "                'billed_charge_local_micro',\n",
    "                'video_views_25',\n",
    "                'video_views_75',\n",
    "                'video_views_50',\n",
    "                'video_views_100',\n",
    "                'video_content_starts']\n",
    "\n",
    "# Copy API response dataframe to Report dataframe\n",
    "report_df = pd.DataFrame(data=dataframe, columns=report_columns)\n",
    "\n",
    "# Reset the index & rename to 'date'\n",
    "report_df.reset_index(inplace=True)\n",
    "report_df = report_df.rename(columns = {'index':'date'})\n",
    "\n",
    "# Apply some formatting to the columns strip leading/trailing whitespaces, titlecase, and replace underscore with ' '\n",
    "report_df.columns = report_df.columns.str.strip().str.title().str.replace('_', ' ')\n",
    "\n",
    "# # Preview Report dataframe\n",
    "# report_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output to csv\n",
    "report_df.to_csv('output.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
